{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a6e1f21-2c31-4adf-b70f-e46ba1ef0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://127.0.0.1:8000/sse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d46196fe-f568-4439-9479-eaa228842799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.mcp import MCPServerSSE\n",
    "from pydantic_ai import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb692685-c844-47d2-8d61-453bb6c83550",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_client = MCPServerSSE(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edf604e8-638f-4ac9-b3b2-9b7753102dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing():\n",
    "    \"\"\"\n",
    "    I don't do anything\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc4b2b29-0f03-4cfc-b3b3-70c5f7159d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call. Use as many \n",
    "keywords from the user question as possible when making first requests.\n",
    "\n",
    "Make multiple searches. Try to expand your search by using new keywords based on the results you\n",
    "get from the search.\n",
    "\n",
    "At the end, make a clarifying question based on what you presented and ask if there are \n",
    "other areas that the user wants to explore.\n",
    "\"\"\".strip()\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=developer_prompt,\n",
    "    tools=[do_nothing],\n",
    "    toolsets=[mcp_client],\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b62c1a12-0a4e-4e64-a980-f347be588b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await agent.run('how do I run kafka in Python?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e04b7f5b-6db7-4fdb-9bc7-437cef93ceed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelRequest(parts=[UserPromptPart(content='how do I run kafka in Python?', timestamp=datetime.datetime(2025, 11, 25, 13, 11, 0, 246202, tzinfo=datetime.timezone.utc))], instructions=\"You're a course teaching assistant. \\nYou're given a question from a course student and your task is to answer it.\\n\\nIf you want to look up the answer, explain why before making the call. Use as many \\nkeywords from the user question as possible when making first requests.\\n\\nMake multiple searches. Try to expand your search by using new keywords based on the results you\\nget from the search.\\n\\nAt the end, make a clarifying question based on what you presented and ask if there are \\nother areas that the user wants to explore.\", run_id='31eb70ab-e970-49a8-9842-e6ad40818b74')\n",
      "\n",
      "ModelResponse(parts=[ToolCallPart(tool_name='search', args='{\"query\":\"run Kafka in Python\"}', tool_call_id='call_9iZGgOKMR7LOTWpxcOUOn8pD')], usage=RequestUsage(input_tokens=321, output_tokens=16, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 11, 25, 13, 11, 1, tzinfo=TzInfo(0)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-Cfn6ras9gQ7QTFbne44n7UxLDsLrg', finish_reason='tool_call', run_id='31eb70ab-e970-49a8-9842-e6ad40818b74')\n",
      "\n",
      "ModelRequest(parts=[ToolReturnPart(tool_name='search', content=[{'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka- python videos have low audio and hard to follow up', 'course': 'data-engineering-zoomcamp'}, {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py', 'course': 'data-engineering-zoomcamp'}, {'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\", 'section': 'Module 6: streaming with kafka', 'question': 'Module “kafka” not found when trying to run producer.py', 'course': 'data-engineering-zoomcamp'}, {'text': \"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\nUse the git bash terminal in windows.\\nActivate python venv from git bash: source .venv/Scripts/activate\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\nNow from git bash, run the seed-kafka cmd. It should work now.\\nAdditional Notes:\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\nThe equivalent of source commands.sh  in Powershell is . .\\\\commands.sh from the workshop directory.\\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\\n—--------------------------------------------------------------------------------------\", 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails', 'course': 'data-engineering-zoomcamp'}], tool_call_id='call_9iZGgOKMR7LOTWpxcOUOn8pD', timestamp=datetime.datetime(2025, 11, 25, 13, 11, 2, 249594, tzinfo=datetime.timezone.utc))], instructions=\"You're a course teaching assistant. \\nYou're given a question from a course student and your task is to answer it.\\n\\nIf you want to look up the answer, explain why before making the call. Use as many \\nkeywords from the user question as possible when making first requests.\\n\\nMake multiple searches. Try to expand your search by using new keywords based on the results you\\nget from the search.\\n\\nAt the end, make a clarifying question based on what you presented and ask if there are \\nother areas that the user wants to explore.\", run_id='31eb70ab-e970-49a8-9842-e6ad40818b74')\n",
      "\n",
      "ModelResponse(parts=[ToolCallPart(tool_name='search', args='{\"query\":\"Kafka Python tutorial\"}', tool_call_id='call_uj6aX7bJnPiakDXAcAJlyNug')], usage=RequestUsage(input_tokens=1292, output_tokens=15, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 11, 25, 13, 11, 3, tzinfo=TzInfo(0)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-Cfn6tuUffNxWwqDQagIdspbT8RGao', finish_reason='tool_call', run_id='31eb70ab-e970-49a8-9842-e6ad40818b74')\n",
      "\n",
      "ModelRequest(parts=[ToolReturnPart(tool_name='search', content=[{'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka- python videos have low audio and hard to follow up', 'course': 'data-engineering-zoomcamp'}, {'text': 'While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\n…\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\n…\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\n…\\nSolution:\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\nSolution 2:\\nCheck what Spark version your local machine has\\npyspark –version\\nspark-submit –version\\nAdd your version to SPARK_VERSION in build.sh', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.', 'course': 'data-engineering-zoomcamp'}, {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py', 'course': 'data-engineering-zoomcamp'}, {'text': 'Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./build.sh: Permission denied Error', 'course': 'data-engineering-zoomcamp'}, {'text': 'Restarting all services worked for me:\\ndocker-compose down\\ndocker-compose up', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py', 'course': 'data-engineering-zoomcamp'}], tool_call_id='call_uj6aX7bJnPiakDXAcAJlyNug', timestamp=datetime.datetime(2025, 11, 25, 13, 11, 3, 956022, tzinfo=datetime.timezone.utc))], instructions=\"You're a course teaching assistant. \\nYou're given a question from a course student and your task is to answer it.\\n\\nIf you want to look up the answer, explain why before making the call. Use as many \\nkeywords from the user question as possible when making first requests.\\n\\nMake multiple searches. Try to expand your search by using new keywords based on the results you\\nget from the search.\\n\\nAt the end, make a clarifying question based on what you presented and ask if there are \\nother areas that the user wants to explore.\", run_id='31eb70ab-e970-49a8-9842-e6ad40818b74')\n",
      "\n",
      "ModelResponse(parts=[TextPart(content='To run Kafka in Python, you typically need to work with a Kafka library specific to Python, the most commonly used being `confluent-kafka`. Here are the basic steps you can follow:\\n\\n1. **Install Required Libraries**: \\n   You can install `confluent-kafka` using pip. Open your terminal and run:\\n   ```bash\\n   pip install confluent-kafka\\n   ```\\n   Alternatively, if you\\'re using conda, you can install it with:\\n   ```bash\\n   conda install conda-forge::python-confluent-kafka\\n   ```\\n\\n2. **Set up a Kafka Environment**: \\n   If you don\\'t already have Kafka running, consider using Docker to set it up for local development. Use a `docker-compose.yml` file or similar setup to simplify the orchestration of Kafka and Zookeeper.\\n\\n3. **Write Your Kafka Producer and Consumer**:\\n   You can create a Kafka producer to send messages and a consumer to receive messages. Here is an example of how this can be done in Python:\\n\\n   **Producer Example**:\\n   ```python\\n   from confluent_kafka import Producer\\n\\n   def delivery_report(err, msg):\\n       if err is not None:\\n           print(f\"Message delivery failed: {err}\")\\n       else:\\n           print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\\n\\n   producer = Producer({\\'bootstrap.servers\\': \\'localhost:9092\\'})\\n   for i in range(10):\\n       producer.produce(\\'my-topic\\', key=str(i), value=f\\'Value {i}\\', callback=delivery_report)\\n       producer.poll(0)\\n\\n   producer.flush()\\n   ```\\n\\n   **Consumer Example**:\\n   ```python\\n   from confluent_kafka import Consumer, KafkaException\\n\\n   consumer = Consumer({\\n       \\'bootstrap.servers\\': \\'localhost:9092\\',\\n       \\'group.id\\': \\'my-group\\',\\n       \\'auto.offset.reset\\': \\'earliest\\'\\n   })\\n\\n   consumer.subscribe([\\'my-topic\\'])\\n\\n   try:\\n       while True:\\n           msg = consumer.poll(1.0)  # timeout of 1 second\\n           if msg is None:\\n               continue\\n           if msg.error():\\n               raise KafkaException(msg.error())\\n           print(f\\'Received message: {msg.value().decode(\"utf-8\")}\\')\\n   finally:\\n       consumer.close()\\n   ```\\n\\n4. **Run your Scripts**: \\n   Ensure that your Kafka server is running and then you can execute your Python scripts to produce and consume messages.\\n\\nDo you have any specific issues or areas of Kafka in Python you want to explore further?')], usage=RequestUsage(input_tokens=2368, cache_read_tokens=1280, output_tokens=531, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 11, 25, 13, 11, 4, tzinfo=TzInfo(0)), provider_name='openai', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-Cfn6uqTi4q6lFicLIrSdsfjUC7Jun', finish_reason='stop', run_id='31eb70ab-e970-49a8-9842-e6ad40818b74')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in result.new_messages():\n",
    "    print(m)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe81aa7-77ca-4d9c-a154-46c61d217883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run Kafka in Python, you typically need to work with a Kafka library specific to Python, the most commonly used being `confluent-kafka`. Here are the basic steps you can follow:\n",
      "\n",
      "1. **Install Required Libraries**: \n",
      "   You can install `confluent-kafka` using pip. Open your terminal and run:\n",
      "   ```bash\n",
      "   pip install confluent-kafka\n",
      "   ```\n",
      "   Alternatively, if you're using conda, you can install it with:\n",
      "   ```bash\n",
      "   conda install conda-forge::python-confluent-kafka\n",
      "   ```\n",
      "\n",
      "2. **Set up a Kafka Environment**: \n",
      "   If you don't already have Kafka running, consider using Docker to set it up for local development. Use a `docker-compose.yml` file or similar setup to simplify the orchestration of Kafka and Zookeeper.\n",
      "\n",
      "3. **Write Your Kafka Producer and Consumer**:\n",
      "   You can create a Kafka producer to send messages and a consumer to receive messages. Here is an example of how this can be done in Python:\n",
      "\n",
      "   **Producer Example**:\n",
      "   ```python\n",
      "   from confluent_kafka import Producer\n",
      "\n",
      "   def delivery_report(err, msg):\n",
      "       if err is not None:\n",
      "           print(f\"Message delivery failed: {err}\")\n",
      "       else:\n",
      "           print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n",
      "\n",
      "   producer = Producer({'bootstrap.servers': 'localhost:9092'})\n",
      "   for i in range(10):\n",
      "       producer.produce('my-topic', key=str(i), value=f'Value {i}', callback=delivery_report)\n",
      "       producer.poll(0)\n",
      "\n",
      "   producer.flush()\n",
      "   ```\n",
      "\n",
      "   **Consumer Example**:\n",
      "   ```python\n",
      "   from confluent_kafka import Consumer, KafkaException\n",
      "\n",
      "   consumer = Consumer({\n",
      "       'bootstrap.servers': 'localhost:9092',\n",
      "       'group.id': 'my-group',\n",
      "       'auto.offset.reset': 'earliest'\n",
      "   })\n",
      "\n",
      "   consumer.subscribe(['my-topic'])\n",
      "\n",
      "   try:\n",
      "       while True:\n",
      "           msg = consumer.poll(1.0)  # timeout of 1 second\n",
      "           if msg is None:\n",
      "               continue\n",
      "           if msg.error():\n",
      "               raise KafkaException(msg.error())\n",
      "           print(f'Received message: {msg.value().decode(\"utf-8\")}')\n",
      "   finally:\n",
      "       consumer.close()\n",
      "   ```\n",
      "\n",
      "4. **Run your Scripts**: \n",
      "   Ensure that your Kafka server is running and then you can execute your Python scripts to produce and consume messages.\n",
      "\n",
      "Do you have any specific issues or areas of Kafka in Python you want to explore further?\n"
     ]
    }
   ],
   "source": [
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d21e2220-15ef-4a4d-a174-d81e6455e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = agent.run_stream('how do I run kafka in Python?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c48104-fed4-4cd0-9ae6-1f3b5d8d828b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
